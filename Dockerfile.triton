# Stage 1: extract only the onnxruntime backend from the full image
FROM nvcr.io/nvidia/tritonserver:23.10-py3 AS backend-src

# Stage 2: minimal Triton server with just what we need
FROM nvcr.io/nvidia/tritonserver:23.10-py3-min

ENV PYTHONUNBUFFERED=1

COPY --from=backend-src /opt/tritonserver/backends/onnxruntime /opt/tritonserver/backends/onnxruntime

WORKDIR /app

COPY --link model_repository/ /models/model_repository/

ENV TRITON_MODEL_REPOSITORY=/models/model_repository

EXPOSE 8000 8001 8002

CMD ["tritonserver", "--model-repository=/models/model_repository"]
