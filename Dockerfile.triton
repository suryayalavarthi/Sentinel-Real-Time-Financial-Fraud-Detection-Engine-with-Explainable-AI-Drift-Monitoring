# Full Triton image â€” the onnxruntime backend links against libcudart and
# libcupti at the .so level, so the -min image cannot satisfy those deps
# without also copying CUDA (which defeats the size savings).
FROM nvcr.io/nvidia/tritonserver:23.10-py3

ENV PYTHONUNBUFFERED=1

WORKDIR /app

COPY --link model_repository/ /models/model_repository/

ENV TRITON_MODEL_REPOSITORY=/models/model_repository

EXPOSE 8000 8001 8002

CMD ["tritonserver", "--model-repository=/models/model_repository"]
